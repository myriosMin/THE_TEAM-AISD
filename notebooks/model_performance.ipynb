{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# File path to the src directory for both linux and windows\n",
    "# workaround for the issue of relative imports in Jupyter notebooks to import modules from src without using the full path\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun this cell after making changes to the utils module\n",
    "from the_team.utils import etl, viz\n",
    "import importlib\n",
    "importlib.reload(etl)\n",
    "importlib.reload(viz)\n",
    "\n",
    "# Set custom plot style for consistency\n",
    "viz.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Base Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "I tried out 4 baseline models that are commonly used for classification tasks.\n",
    "- Logistic Regression: A simple yet highly interpretable baseline that works well for binary classification\n",
    "- Random Forest: Go-to model for complex classification tasks, aptures non-linear patterns and ranks feature importance, making it useful for understanding buyer behavior.\n",
    "- XGBoost: Strong performance on tabular data, especially with imbalanced classes\n",
    "- LightGBM: Fast and scalable, making it efficient for large datasets with mixed features and iterative tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_path = Path(\"../data/08_reporting/random_forest_model_metrics.json\")\n",
    "lr_path = Path(\"../data/08_reporting/logistic_model_metrics.json\")\n",
    "xg_path = Path(\"../data/08_reporting/xgboost_model_metrics.json\")\n",
    "lgbm_path = Path(\"../data/08_reporting/lightgbm_model_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = etl.load_model_metrics(rf_path)\n",
    "lr = etl.load_model_metrics(lr_path)\n",
    "xg = etl.load_model_metrics(xg_path)\n",
    "lgbm = etl.load_model_metrics(lgbm_path)\n",
    "models = {\"Random Forest\": rf, \"Logistic Regression\": lr, \"XGBoost\": xg, \"LightGBM\": lgbm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw accuracies\n",
    "raw_accuracies = pd.DataFrame({\n",
    "    \"Random Forest\": rf[\"classification_report\"][\"accuracy\"],\n",
    "    \"Logistic Regression\": lr[\"classification_report\"][\"accuracy\"],\n",
    "    \"XGBoost\": xg[\"classification_report\"][\"accuracy\"],\n",
    "    \"LightGBM\": lgbm[\"classification_report\"][\"accuracy\"]\n",
    "}, index=[\"Accuracy\"])\n",
    "raw_accuracies.plot(kind=\"bar\", figsize=(5, 3), title=\"Raw Model Accuracies\", ylabel=\"Accuracy\", xlabel=\"Models\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- The accuracies can still be improved but are acceptable. \n",
    "- However, the training data has huge class imbalance (6% True, 94% False).\n",
    "- Even if the model say False to every instance, the accuray would still achieve 94% accuracy.\n",
    "- Thus, overall accuracy (as well as ROC-AUC) metrics are not so suitable in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_metrics in models.items():\n",
    "    viz.plot_classification_report(model_metrics[\"classification_report\"], model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- As seen above, the model is good at identifying False (non-repeat buyers) instances but missing out on True (repeat buyer) classes.\n",
    "- Although class weights and scaling were already used during training these base models, there is still much room for improvement, possible through hyperparamter tuning and probability thresholding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Logistic Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "- Logistic regression, which has slightly better metrics and very explainable, is chosen to prove the feasability of model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fine_tuned = etl.load_model_metrics(Path(\"../data/08_reporting/logistic_model_tuning_metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base threshold has been changed to {lr_fine_tuned['best_threshold']:.2f} after finetuning.\")\n",
    "# The model is now more sensitive to positive class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vs_fine_tuned = {\"Logistic Regression\": lr, \"Finetuned Logistic Regression\": lr_fine_tuned}\n",
    "\n",
    "viz.plot_before_after_metrics(lr_vs_fine_tuned, \"Fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "- Precision, in our case, indicates how many of our model's predicted repeat buyers are acutal repeat buyers. This is useful in targeted marketing campaigns where high precision means that the customers we target are truly likely to buy again, reducing wasted marketing effort.\n",
    "- Thus, precison was used as the target metric during hyperparameter tuning.\n",
    "- Precision increased by 7% but recall was sacrificed in the process although the overall f1 score still increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top10 precision beforeand after finetuning\n",
    "print(f\"Before finetuning, the top 10 precision scores was: {lr['top_10_precision']:.2f}\")\n",
    "print(f\"After finetuning, the top 10 precision scores is: {lr_fine_tuned['top_10_precision']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "- Not much difference, but is surprisingly high for such an imbalanced dataset. \n",
    "- 20% in top-10 precision means that, among the top 10 customers ranked most likely to be repeat buyers by our model, 2 of them are actually repeat buyers.(There might be other POTENTIAL repeat buyers in that top 10 customers as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "- Our main business goal was to identify POTENTIAL repeat buyers, and all of our features were engineered towards it. \n",
    "- But, our flag for is_repeat_buyer is defined as customers who have more than once unique purchases within the whole provided dataset, meaning they are existing repeat buyers. \n",
    "- Thus, when our model predicts a buyer as a repeat buyer, the person, at that point in time, might not have become a repeat buyer but had potential. Yet, since the flag was False, the model was told wrong, accounting for low precision. \n",
    "- Therefore, we are trying out semi-supervised learning, for instances, where we logically think the customer may buy again, but we cannot say for sure: pseudo labels or weak labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (\n",
    "#         (df[\"review_score\"] > 3)\n",
    "#         | (df[\"deli_duration_exp\"] <= -7)\n",
    "#         | (df[\"voucher\"] >= 0.3)\n",
    "#         | (df[\"total_spent\"] >= df[\"total_spent\"].quantile(0.8))\n",
    "#         | (df[\"product_category_name\"].isin(top_categories))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "These features are based on domain expertise like\n",
    "- if the customer is satisfied (review > 3), the person might buy again, or\n",
    "- if the customer paid 30% of the total spent in vouchers, the person is knowledgeable about Olist platforms (coupons, loyalty points) and might buy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_lr = etl.load_model_metrics(Path(\"../data/08_reporting/ssl_logistic_model_metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_all = {\"Logistic Regression\": lr, \"Finetuned Logistic Regression\": lr_fine_tuned, \"SSL Logistic Regression\": ssl_lr}\n",
    "viz.plot_before_after_metrics(lr_all, \"Semi-supervised Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "- The model has improved so much that it's too good to be true now. \n",
    "- But this feasability proves that semi-supervised learning may work in our case of predicting POTENTIAL repeat buyer, where the goal is about a weak label. (There is no such thing as a potential repeat buyer in the provided dataset.)\n",
    "- How well the SSL works largely depends on defining the correct masking pesudo labels without much bias, and this can be further improved when the label actually becomes True (from potential to actual repeat buyer) in the future through continuous reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PRC curves for all lr models\n",
    "plt.figure(figsize=(6, 6))\n",
    "for model_name, result in lr_all.items():\n",
    "    precision = result[\"prc_curve\"][\"precision\"]\n",
    "    recall = result[\"prc_curve\"][\"recall\"]\n",
    "    auc = result[\"prc_auc\"]\n",
    "    label = f\"{model_name} (PRC-AUC = {auc:.3f})\"\n",
    "    plt.plot(recall, precision, label=label)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (All Logistic Models)\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Model\", loc=\"lower left\", bbox_to_anchor=(0, -0.7))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "(SSL model is way too optimistic and should be configured with above suggestions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_bias = pd.read_csv(Path(\"../data/08_reporting/ssl_bias_report.csv\"))\n",
    "ssl_bias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "- Out of 93617 repeat buyers, 61154 (~65%) came from the top 10 categories (which we intentionally defined earlier for semi-supervised learning). \n",
    "- This indicates that the pseudo-labeling model might be biased. It may have learned to assign “repeat buyer” labels primarily based on category frequency, rather than user behavior. \n",
    "- This risks overfitting to popular products, and failing on underrepresented or niche categories.\n",
    "- Countermeaures would be to downweight product categories during pseudo-labelling or adding more diversity.\n",
    "(This same appraoch can be used to check model biasness for each of the new conditions we defined during pesudo-labelling.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
