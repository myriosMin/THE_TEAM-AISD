{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Unidecode\n",
    "# !pip install datashader\n",
    "# !pip install geopandas\n",
    "# !pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from unidecode import unidecode\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# File path to the src directory for both linux and windows\n",
    "# workaround for the issue of relative imports in Jupyter notebooks to import modules from src without using the full path\n",
    "src_path = os.path.abspath(\"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun this cell after making changes to the utils module\n",
    "from the_team.utils import etl, viz\n",
    "import importlib\n",
    "importlib.reload(etl)\n",
    "importlib.reload(viz)\n",
    "\n",
    "# Set custom plot style for consistency\n",
    "viz.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = Path(\"data\") / \"01_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "customers = etl.load_csv(RAW_DIR / \"olist_customers_dataset.csv\")\n",
    "geolocation = etl.load_csv(RAW_DIR / \"olist_geolocation_dataset.csv\")\n",
    "items = etl.load_csv(RAW_DIR / \"olist_order_items_dataset.csv\")\n",
    "payments = etl.load_csv(RAW_DIR / \"olist_order_payments_dataset.csv\")\n",
    "reviews = etl.load_csv(RAW_DIR / \"olist_order_reviews_dataset.csv\")\n",
    "orders = etl.load_csv(RAW_DIR / \"olist_orders_dataset.csv\")\n",
    "products = etl.load_csv(RAW_DIR / \"olist_products_dataset.csv\")\n",
    "sellers = etl.load_csv(RAW_DIR / \"olist_sellers_dataset.csv\")\n",
    "translation = etl.load_csv(RAW_DIR / \"product_category_name_translation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Geolocation-related datasets [Jin Bin]\n",
    "- customers.csv\n",
    "- geolocation.csv\n",
    "- sellers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "etl.null_duplicate_check(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "formatted_customers = etl.format_customers(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min seems low but its not a problem \n",
    "formatted_customers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_top_locations(formatted_customers, title_prefix=\"Customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "1. Strong Regional Concentration.(SP) dominates customer count with over 40,000 customers — nearly half of the data. This suggests regional market dependence so if Olist wants to target repeat buyers, SP should be a priority.\n",
    "\n",
    "2. Urban Centers Drive Volume. Cities like São Paulo, Rio de Janeiro, and Belo Horizonte are far ahead of others. (Urban hubs = higher density = possibly faster repeat behavior.) --> we could analyze if urban customers reorder more frequently due to better delivery coverage or seller availability after taking sellers.csv into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### geolocation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The geolocation dataset contains multiple similar latitude and longitude entries for the same zip code prefix. To simplify the data and enable efficient merging with customer and seller datasets, we averaged the latitude and longitude for each unique zip code prefix. While this reduces geographic precision, it preserves regional location context needed for distance-based analysis in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks for duplicates\n",
    "etl.null_duplicate_check(geolocation, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "max lat/lng values are a little suspicious cause the borders of brazil are not that big\n",
    "- lat range should be between 33.75116944 and 5.27438888\n",
    "- lng range should be between -73.98283055 and -34.79314722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Brazil's boundary conditions\n",
    "valid_lat_range = (-33.75116944, 5.27438888)\n",
    "valid_lng_range = (-73.98283055, -34.79314722)\n",
    "\n",
    "# Prepare data\n",
    "lat_outliers = geolocation[\n",
    "    (geolocation[\"geolocation_lat\"] < valid_lat_range[0]) |\n",
    "    (geolocation[\"geolocation_lat\"] > valid_lat_range[1])\n",
    "][\"geolocation_lat\"]\n",
    "\n",
    "lng_outliers = geolocation[\n",
    "    (geolocation[\"geolocation_lng\"] < valid_lng_range[0]) |\n",
    "    (geolocation[\"geolocation_lng\"] > valid_lng_range[1])\n",
    "][\"geolocation_lng\"]\n",
    "\n",
    "# Plot base boxplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=geolocation[[\"geolocation_lat\", \"geolocation_lng\"]], orient=\"h\", palette=\"Set2\")\n",
    "\n",
    "# Overlay red dots for out-of-bound lat/lng\n",
    "plt.scatter(lat_outliers, [\"geolocation_lat\"] * len(lat_outliers), color=\"red\", label=\"Out-of-Range\")\n",
    "plt.scatter(lng_outliers, [\"geolocation_lng\"] * len(lng_outliers), color=\"red\")\n",
    "\n",
    "plt.title(\"Boxplot of Latitude and Longitude (Geolocation)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier Lat/Lng values \n",
    "# Filter only valid rows\n",
    "geolocation = geolocation[\n",
    "    (geolocation[\"geolocation_lat\"].between(*valid_lat_range)) &\n",
    "    (geolocation[\"geolocation_lng\"].between(*valid_lng_range))\n",
    "]\n",
    "\n",
    "# Check if any outliers remain\n",
    "print(f\"Remaining rows: {len(geolocation)}\")\n",
    "geolocation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_geolocation = etl.format_geolocation(geolocation)\n",
    "formatted_geolocation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "will be merged with customers.csv and sellers.csv to plot a map for the distribution of sellers and customers. Could also be used to calculate distance between the 2 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### sellers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "etl.null_duplicate_check(sellers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sellers = etl.format_sellers(sellers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sellers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_top_locations(formatted_sellers, \n",
    "                   state_col=\"seller_state\", \n",
    "                   city_col=\"seller_city\", \n",
    "                   title_prefix=\"Seller\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Customer and seller geographic distribution shows strong overlap in urban regions such as São Paulo and Rio de Janeiro. This urban concentration, combined with higher seller density, could facilitate quicker deliveries and higher satisfaction—factors known to influence repeat buying behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Review-related datasets [Habib]\n",
    "- review.csv\n",
    "- products.csv\n",
    "- translation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .info shows missing entries\n",
    "etl.null_duplicate_check(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Since many users about 90% did not give their comments a title I will drop the title column as the data is not complete enough, the review date/timestamp are not relevant as \n",
    "# there is no time aspect in this situation so that will be dropped as well. The Review Id is also not required as its just used to index the reviews and this is not relevant.\n",
    "\n",
    "reviews.drop(columns=[\"review_id\", \"review_comment_title\",\"review_creation_date\", \"review_answer_timestamp\"], inplace=True)\n",
    "\n",
    "reviews.head()\n",
    "\n",
    "# Dataset is now less bloated and only relevant columns remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the spread of the review scores before any validation.\n",
    "\n",
    "viz.plot_categorical_distribution(reviews, \"review_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "- We can see that there is a large bias towards 5 & 4 point reviews suggesting about 1 in 3 orders are satisfactory and leave a posititve impression on the end user.\n",
    "- However there is a large amount of neutral reviews. To prevent non useful datapoints, this will be checked against the sentiment of the comments to verify all ratings are as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### Products dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .info shows small amount of missing entries\n",
    "etl.null_duplicate_check(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) There is no possible way to feature engineer or substitute placeholder values for any of the columns so I will drop any df row with no data entries\n",
    "\n",
    "products.dropna(inplace=True)\n",
    "\n",
    "etl.null_duplicate_check(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Convert float to int for certain columns\n",
    "\n",
    "products = products.astype({\n",
    "    \"product_name_lenght\": int,\n",
    "    \"product_description_lenght\": int,\n",
    "    \"product_photos_qty\": int\n",
    "})\n",
    "\n",
    "# Checking all dt conversions worked as intended\n",
    "products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation.head()\n",
    "# From what I can see, this is a small dataset for end users to translate the portuguese labeled items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation.info()\n",
    "# There are no missing or duplicate values. Nor any further visualisations, so I will leave this dataset as is and may use it in the future for sentiment analysis of product categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Order-related datasets [Min]\n",
    "- items.csv\n",
    "- payments.csv\n",
    "- orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df info says no Null but still; Null and duplicate check\n",
    "etl.null_duplicate_check(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. shipping_limit_date has wrong data type.\n",
    "items[\"shipping_limit_date\"] = etl.to_datetime(items[\"shipping_limit_date\"])\n",
    "assert items[\"shipping_limit_date\"].dtype == \"datetime64[ns]\", \"shipping_limit_date should be datetime64[ns]\"\n",
    "\n",
    "# order_item_id is categircal, so object type is fine\n",
    "\n",
    "# Some duplicates are expected after dropping order_item_id, since there can be multiple items in an order.\n",
    "# Left untreated as these duplicates are identifiable and workable with the order_id.\n",
    "etl.null_duplicate_check(items, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of numeric columns\n",
    "items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "- A large gap between the 75th percentile (Q3) and the maximum in prices indicates that there are high-value outliers. \n",
    "- Olist dataset distributor mentioned (in Kaggle) that the data is from 2016 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop rows not within 2016-2018\n",
    "items = items[items[\"shipping_limit_date\"].dt.year.isin([2016, 2017, 2018])]\n",
    "\n",
    "# Plot distribution of numeric columns\n",
    "viz.plot_numeric_distribution(items.drop(\"order_item_id\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "- Both price & freight_value columns show strong right-skewed distributions with a large number of high-value outliers, which is expected as people mostly buy FMCGs (with low cost) from online, not high-end products.\n",
    "- Right-skewed price means the item is expensive while right-skewed freight_value means either the customer lives far away from sellers or the item is physically huge; this can be cross-checked later with geolocation and product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_99p = items['price'].quantile(0.999)\n",
    "print(f\"99.9% of price is ${price_99p}.\")\n",
    "freight_99p = items['freight_value'].quantile(0.999)\n",
    "print(f\"99.9% of freight_value is ${freight_99p:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "High values in both price and freight_value are important as they may suggest:\n",
    "- People who bought expensive quality producuts are less likely to buy again. \n",
    "- People who have to pay high freight_value are less likely to buy again. \n",
    "But, capturing 99.9% of current customers should be representative enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove outliers\n",
    "items['price'] = etl.cap_outliers(items['price'], min_cap=False, max_cap=.999)\n",
    "items['freight_value'] = etl.cap_outliers(items['freight_value'], min_cap=False, max_cap=.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Flag high values for the model to learn that a price/freight is unusually high (usual outliers) \n",
    "items['price_high'] = items['price'] > items['price'].quantile(0.75) * 1.5\n",
    "items['freight_value_high'] = items['freight_value'] > items['freight_value'].quantile(0.75) * 1.5\n",
    "items.freight_value_high.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### orders.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to datetime and clip to 2016-2018\n",
    "for col in orders.columns[3:]:\n",
    "    orders[col] = etl.clip_datetime(orders[col])\n",
    "    assert orders[col].dtype == \"datetime64[ns]\", f\"{col} should be datetime64[ns]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "- order_delivered_carrier_date (which marks when the seller handed the package to the carrier) is mainly for logistic purposes and does not influence repeat buyer behaviour as comparied to the duration between purchase and delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop order_delivered_carrier_date\n",
    "orders.drop(columns=[\"order_delivered_carrier_date\"], inplace=True)\n",
    "assert \"order_delivered_carrier_date\" not in orders.columns, \"order_delivered_carrier_date should be dropped\"\n",
    "\n",
    "# Null values\n",
    "etl.null_duplicate_check(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3% of the data shows that the order_delivered_customer_date is null\n",
    "# meaning that the order was not delivered or have not been delivered yet to the customer\n",
    "\n",
    "# Check if null dates are related to different order statuses\n",
    "null_dates_orders = orders[orders.isnull().any(axis=1)].copy()\n",
    "viz.plot_categorical_distribution(null_dates_orders, \"order_status\", \"Count of null dates by order status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "- Since the goal is to identify potential repeat buyer, we will focus on 'paid' statuses; dropping others while trying to impute the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop rows with [canceled, created, invoiced, unavailable] order_status\n",
    "orders = orders[~orders[\"order_status\"].isin([\"canceled\", \"created\", \"invoiced\", \"unavailable\"])]\n",
    "\n",
    "# Vislualize the duration between order_approved_at and order_delivered_customer_date\n",
    "mean, median, mode = viz.plot_duration_distribution(\n",
    "                        df=orders,\n",
    "                        column_x='order_delivered_customer_date',\n",
    "                        column_y='order_estimated_delivery_date',\n",
    "                        title='Delivery Duration Distribution between Estimated and Actual Delivery Dates',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "- The duration is right-skewed, and the mean is dragged towards right due to extreme outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fill null values in order_delivered_customer_date with the median duration\n",
    "orders[\"order_delivered_customer_date\"] = orders[\"order_delivered_customer_date\"].fillna(orders[\"order_estimated_delivery_date\"] - median) # type: ignore\n",
    "assert orders[\"order_delivered_customer_date\"].isnull().sum() == 0, \"There should be no null values in order_delivered_customer_date\"\n",
    "\n",
    "# For null order_approved_at,\n",
    "orders[orders.order_approved_at.isna()]['order_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "- order_status shows delivered, but order_approved_at (payment time) and order_delivered_customer_date is null, meaning these are mix-mactched; incorrect data (maybe the customer used points to exchange instead of payment?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. drop NULL in order_approved_at\n",
    "orders.dropna(inplace=True)\n",
    "assert orders.isnull().sum().sum() == 0, \"There should be no null values in the dataset as of now.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "#### payments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "payments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They are all in correct data types.\n",
    "etl.null_duplicate_check(payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems the school intentionally screw the data btw ^^\n",
    "payments.order_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which payment types are commonly used?\n",
    "viz.plot_categorical_distribution(payments, \"payment_type\", \"Count of Payment Types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glance at installments\n",
    "viz.plot_numeric_distribution(payments.drop(columns=[\"order_id\", \"payment_sequential\", \"payment_type\", \"payment_value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "- There is no info on not_defined payment type and logically does not make sense; dropped.\n",
    "- payment_installments is right-skewed, and that many times of installments are not common either; capped.\n",
    "- payment_sequential is based on customer preference and is redundant; we can work with groupby later if necessary; dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. drop not_defined payment types\n",
    "payments = payments[payments[\"payment_type\"] != \"not_defined\"]\n",
    "assert \"not_defined\" not in payments[\"payment_type\"].unique(), \"not_defined payment type should be dropped\"\n",
    "\n",
    "# 2. cap installments to 99% quantile\n",
    "payments[\"payment_installments\"] = etl.cap_outliers(payments[\"payment_installments\"], min_cap=False, max_cap=True)\n",
    "\n",
    "# 3. drop payment_sequential as it is not relevant for the analysis; payment_value has been analyzed in the items dataset, so we can drop it here\n",
    "payments.drop(columns=[\"payment_sequential\"], inplace=True)\n",
    "assert \"payment_sequential\" not in payments.columns, \"payment_sequential should be dropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = payments.groupby(\"order_id\")[\"payment_installments\"].nunique()\n",
    "multiple_installments = df[df > 2].index.values\n",
    "payments[payments[\"order_id\"].isin(multiple_installments[:1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "- People are weird; for the same order, he chose to pay with a voucher, a credit card with 4 installments and another credit card with 6 installments. \n",
    "- Since this is also a part of Olist's flexible payment, these installments will just be added together to reduce complexity while keeping info."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
